asas

"Computers are stupdi......    
THey can only give you answers"       
-- Pablo Picasso

Can we deisign techniogy tohelp us ask better questionsa bout the world?

Answer1: this is not a question for technologisits.  That is someone else's
problem.


Answer2: This is absolutely our problem. Our house is on fire and we need do
something about it

- Automatoc Recidivism models are more likely to 
[falsely label black defendants as future criminals](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
 at twice the rate as white defen-
dants.
- Widely-used facial recognition software that predicts char-
acteristics, such as gender and age, has a 
[much higher error rate for dark-skinned women compared to light-skinned
men](https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212)
- Amazon’s ML software to offer same-day delivery to prime
users became [biased against black neighborhoods](https://www.businessinsider.com/how-algorithms-can-be-racist-2016-4)
- Google Translate, the most popular translation engine in
the world, [shows gender bias](https://www.science.org/doi/10.1126/science.aal4230). 
“She is an engineer, He is a
nurse” is translated into Turkish and then again into English
becomes “He is an engineer, She is a nurse” [27].
- For more examples, see 
[Kugler'22](https://cacm.acm.org/magazines/2022/4/259390-technologys-impact-on-morality/fulltext) or
[Rudin,'19](https://arxiv.org/pdf/1811.10154.pdf) or
[Gebru'19](https://arxiv.org/pdf/1908.06165.pdf) or
[Nobel'18](https://www.biblio.com/9781479837243).

Answer3: "We" (i.e. engineers) are the wrong people to solve this.
Technology has alsways been selected to gavour the ruling elite.
ogy and, as such, they are selected to [favor the ruling elite](https://www.google.com/books/edition/America_by_Design/LBYlAV6VmpwC?hl=en&gbpv=0)
In
this view, algorithms are as inherently as bad (racist, sexist, extrem-
ist, misinforming) as anything else selected by their social context.
Hence there is no value in fixing algorithms until we first fix the
society that selects and deploys them [^1]


Answer4: We (engieers) cuased the problem. We are responsible. We need to take action. 

> 
"Technical decisions end up shaping the choices we make about how to present ourselves, 
which in turn shapes how we view ourselves and other people."...   
"Social media platforms can serve us content that enrages or
depresses us, making it more (or less) likely we will take immoral
actions based on our feelings. These platforms also can be used by
bad actors to take immoral actions more easily."    
"The same functionality that allows moral choices also enables immoral ones, as 
Facebook whistle- blower Francis Haugen told the U.S. Congress. Haugen testified in 
October 2021 that Facebook knowingly served content containing hate speech and misinformation 
to its users, since that increased engagement. Some of the users receiving that content then 
decided to speak and act in hateful ways, which caused mental and physical harm to others. 
In one scenario, Haugen said, the company's technology was even used to fan the flames of genocide in Myanmar, literally costing lives.   
-- [Kugler'22](https://cacm.acm.org/magazines/2022/4/259390-technologys-impact-on-morality/fulltext),


[^1]: And such social-level fixes are going to need a  lot of work from a lot of people.  e.g. [on page 179 of Nobel'18](https://www.biblio.com/9781479837243). such fixes are listes as "decoupling of advertising and commercial interests from the ability to access high-quality information on the Internet”, “suspend the circulation of racist and sexist material that is used to erode our civil and human rights”; and require that all search results be annotated to symbolize e.g. pornography (in red), business or commercial material (in green), entertainment (in orange), etc.
